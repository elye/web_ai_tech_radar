---
name: "Grok"
organization: "xAI"
ring: "trial"
quadrant: "models"
tags: ["llm", "moe", "multimodal", "xai", "open-weights", "reasoning"]
date: "2024-03-15"
featured: true
cost: "freemium"
draft: false
---

## Overview
Grok is a state-of-the-art large language model (LLM) developed by xAI, designed for real-time reasoning, agentic workflows, and multimodal understanding. It leverages a Mixture-of-Experts (MoE) architecture with 314B parameters, supporting advanced tool use and real-time search integration. Grok is available via API, web, iOS, Android, and enterprise plans, with open weights for research and development.

## Key Benefits
- State-of-the-art performance on reasoning and coding tasks
- Real-time search and native tool use
- Multimodal capabilities (text, code, images)
- Open weights for transparency and reproducibility
- Flexible deployment: API, web, mobile, enterprise
- High scalability via MoE architecture

## When to Use
- When you need advanced reasoning or coding capabilities
- For real-time information retrieval and agentic workflows
- In research or enterprise settings requiring transparency and reproducibility
- For multimodal analysis (text, code, images)


## Cost
**Freemium**: Grok is available under a freemium model.
- Free tier: Limited access via web and API
- Paid tiers: SuperGrok, Premium+, and Enterprise plans with higher rate limits and access to Grok 4 Heavy
- Open weights: Free for research and development (Apache 2.0 license)

## Key Features
- Mixture-of-Experts (MoE) architecture (8 experts, 2 per token)
- 314B parameters, 64 layers, 48 attention heads
- Multimodal: text, code, images (varies by version)
- Real-time search and native tool use
- Supports activation sharding, 8-bit quantization
- Maximum context: 8,192 tokens
- Open weights for research (Apache 2.0 license)
- Maximum context: 8,192 tokens
- Enterprise AI applications
- Research and benchmarking

## Strengths
- State-of-the-art performance on reasoning and coding tasks
- Open weights for transparency and reproducibility
- Flexible deployment: API, web, mobile, enterprise
- High scalability via MoE architecture

## Limitations
- Requires significant GPU resources for local inference (314B params)
- Some advanced features (Grok 4 Heavy, SuperGrok) require paid subscription
- Multimodal capabilities may vary by version

## Resources
- [Official website](https://grok.x.ai/)
- [API console](https://console.x.ai/)
- [Developer docs](https://docs.x.ai/)
- [Grok 4 news](https://x.ai/news/grok-4)
- [GitHub: Grok-1 open weights](https://github.com/xai-org/grok-1)
- [Grok pricing/plans](https://grok.com/plans)
- [arXiv: ConsistencyAI benchmark (Grok-3)](https://arxiv.org/abs/2510.13852)
- [arXiv: Grok-4 Heavy SOTA](https://arxiv.org/abs/2510.08591)
- [arXiv: Multimodal Grok MLLM](https://arxiv.org/abs/2510.04281)
- [arXiv: Grokking phase transition](https://arxiv.org/abs/2509.21519)
